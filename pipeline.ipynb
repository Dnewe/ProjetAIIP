{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acdb81c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T18:08:25.545672Z",
     "start_time": "2025-12-01T18:08:11.117268Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "### fix old numpy code in LVISEval (np.float is deprecated)\n",
    "if not hasattr(np, \"float\"):\n",
    "    np.float = float\n",
    "import torch\n",
    "import torchvision\n",
    "import requests\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import List, Tuple, Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2228f380",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T18:08:25.864994Z",
     "start_time": "2025-12-01T18:08:25.547526Z"
    }
   },
   "outputs": [],
   "source": [
    "### add this if on colab:\n",
    "# !pip install lvis \n",
    "### remove if on colab:\n",
    "from constants import *\n",
    "import lvis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911e1043",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb267844",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T18:08:25.913503Z",
     "start_time": "2025-12-01T18:08:25.866931Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import decode_image\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.ops import box_convert\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "### with help of https://docs.pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n",
    "\n",
    "\n",
    "class LVISDataset(Dataset):\n",
    "    \"\"\" \n",
    "    Dataset class to load data from the disk or URLs.\n",
    "    Transform annotations into the format required by pytorch.\n",
    "\n",
    "    img_dirs : List of paths where images are stored.\n",
    "    lvis_gt : LVIS object containing annotations (ground truth).\n",
    "    transforms (optional) : torchvision.transforms composition.\n",
    "    cat_ids (optional) : category ids to filter.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_dirs: List[str], lvis_gt: lvis.LVIS, transforms=None, cat_ids=None) -> None:\n",
    "        self.img_dirs = img_dirs\n",
    "        self.lvis_gt = lvis_gt\n",
    "        self.transforms = transforms\n",
    "        self._create_index(cat_ids)\n",
    "\n",
    "    def _create_index(self, cat_ids):\n",
    "        \"\"\"\n",
    "        Maps the indices of the dataset with the LVIS indices.\n",
    "        \"\"\"\n",
    "        # Filter images based on the provided ids or get all images.\n",
    "        self.img_ids = self.lvis_gt.get_img_ids() if cat_ids is None else self._get_img_ids(cat_ids)\n",
    "        self.cat_ids = self.lvis_gt.get_cat_ids() if cat_ids is None else cat_ids\n",
    "\n",
    "        # Mapping id to label in both ways.\n",
    "        # i+1 because id = 0 is for background.\n",
    "        self.cat_id_to_label = {cat_id: i + 1 for i, cat_id in enumerate(self.cat_ids)}\n",
    "        self.label_to_cat_id = {i + 1: cat_id for i, cat_id in enumerate(self.cat_ids)}\n",
    "\n",
    "        # Loads from local files or from URL.\n",
    "        if all(isinstance(dir, str) for dir in self.img_dirs) and all(os.path.isdir(dir) for dir in self.img_dirs):\n",
    "            self._get_image = self._get_image_from_file\n",
    "            print(\"will load images from files\")\n",
    "        else:\n",
    "            self._get_image = self._get_image_from_url\n",
    "            print(\"will load images from urls\")\n",
    "\n",
    "    def _get_img_ids(self, cat_ids):\n",
    "        \"\"\"\n",
    "        Filter images based on the provided ids or get all images.\n",
    "        \"\"\"\n",
    "        return list({\n",
    "            iid for cat_id in cat_ids\n",
    "            for iid in self.lvis_gt.cat_img_map[cat_id]\n",
    "        })\n",
    "\n",
    "    def _get_image_from_file(self, id):\n",
    "        \"\"\"\n",
    "        Loads an image from the disk and returns a torch.Tensor\n",
    "        \"\"\"\n",
    "        image_paths = [os.path.join(images_dir, f'{str(id).zfill(12)}.jpg') for images_dir in self.img_dirs]\n",
    "        for image_path in image_paths:\n",
    "            if os.path.isfile(image_path):\n",
    "                return decode_image(image_path)\n",
    "        print(f\"image not found: {image_paths}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    def _get_image_from_url(self, id):\n",
    "        \"\"\"\n",
    "        Dowloads an image from the COCO website\n",
    "        \"\"\"\n",
    "        url = self.lvis_gt.imgs[id]['coco_url']\n",
    "        response = requests.get(url)\n",
    "        img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"\n",
    "        Returns the image (as a Tensor) and targets (as a Dict) for the specified index idx.\n",
    "        \"\"\"\n",
    "        ### get image\n",
    "        img_id = self.img_ids[idx]\n",
    "        img = self._get_image(img_id)\n",
    "        img = tv_tensors.Image(img)\n",
    "        _, h, w = img.shape\n",
    "\n",
    "        ### get annotationss\n",
    "        annot_ids = self.lvis_gt.get_ann_ids(img_ids=[img_id])\n",
    "        annots = self.lvis_gt.load_anns(annot_ids)\n",
    "        annots = [annot for annot in annots if annot['category_id'] in self.cat_ids]\n",
    "        # labels\n",
    "        labels = torch.tensor([self.cat_id_to_label[annot['category_id']] for annot in annots])\n",
    "        # area\n",
    "        areas = torch.tensor([annot['area'] for annot in annots])\n",
    "        # boxes\n",
    "        boxes = torch.tensor([annot['bbox'] for annot in annots], dtype=torch.float32)\n",
    "        boxes_xyxy = box_convert(boxes, in_fmt='xywh', out_fmt='xyxy')\n",
    "        boxe_tv = tv_tensors.BoundingBoxes(boxes_xyxy, format='XYXY', canvas_size=(h, w))  # type: ignore\n",
    "        # masks\n",
    "        masks = [torch.from_numpy(self.lvis_gt.ann_to_mask(ann)) for ann in annots]  # shape: (N, H, W)\n",
    "        mask_tv = tv_tensors.Mask(torch.stack(masks))\n",
    "\n",
    "        target = {}\n",
    "        target['image_id'] = img_id\n",
    "        target['labels'] = labels\n",
    "        target['area'] = areas\n",
    "        target['boxes'] = boxe_tv\n",
    "        target['masks'] = mask_tv\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1557b52b",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9d104e8e77f8d4",
   "metadata": {},
   "source": [
    "    Args maskRCNNN_resnet50(:\n",
    "        backbone (nn.Module): the network used to compute the features for the model.\n",
    "            It should contain an out_channels attribute, which indicates the number of output\n",
    "            channels that each feature map has (and it should be the same for all feature maps).\n",
    "            The backbone should return a single Tensor or and OrderedDict[Tensor].\n",
    "        num_classes (int): number of output classes of the model (including the background).\n",
    "            If box_predictor is specified, num_classes should be None.\n",
    "        min_size (int): Images are rescaled before feeding them to the backbone:\n",
    "            we attempt to preserve the aspect ratio and scale the shorter edge\n",
    "            to ``min_size``. If the resulting longer edge exceeds ``max_size``,\n",
    "            then downscale so that the longer edge does not exceed ``max_size``.\n",
    "            This may result in the shorter edge beeing lower than ``min_size``.\n",
    "        max_size (int): See ``min_size``.\n",
    "        image_mean (Tuple[float, float, float]): mean values used for input normalization.\n",
    "            They are generally the mean values of the dataset on which the backbone has been trained\n",
    "            on\n",
    "        image_std (Tuple[float, float, float]): std values used for input normalization.\n",
    "            They are generally the std values of the dataset on which the backbone has been trained on\n",
    "        rpn_anchor_generator (AnchorGenerator): module that generates the anchors for a set of feature\n",
    "            maps.\n",
    "        rpn_head (nn.Module): module that computes the objectness and regression deltas from the RPN\n",
    "        rpn_pre_nms_top_n_train (int): number of proposals to keep before applying NMS during training\n",
    "        rpn_pre_nms_top_n_test (int): number of proposals to keep before applying NMS during testing\n",
    "        rpn_post_nms_top_n_train (int): number of proposals to keep after applying NMS during training\n",
    "        rpn_post_nms_top_n_test (int): number of proposals to keep after applying NMS during testing\n",
    "        rpn_nms_thresh (float): NMS threshold used for postprocessing the RPN proposals\n",
    "        rpn_fg_iou_thresh (float): minimum IoU between the anchor and the GT box so that they can be\n",
    "            considered as positive during training of the RPN.\n",
    "        rpn_bg_iou_thresh (float): maximum IoU between the anchor and the GT box so that they can be\n",
    "            considered as negative during training of the RPN.\n",
    "        rpn_batch_size_per_image (int): number of anchors that are sampled during training of the RPN\n",
    "            for computing the loss\n",
    "        rpn_positive_fraction (float): proportion of positive anchors in a mini-batch during training\n",
    "            of the RPN\n",
    "        rpn_score_thresh (float): only return proposals with an objectness score greater than rpn_score_thresh\n",
    "        box_roi_pool (MultiScaleRoIAlign): the module which crops and resizes the feature maps in\n",
    "            the locations indicated by the bounding boxes\n",
    "        box_head (nn.Module): module that takes the cropped feature maps as input\n",
    "        box_predictor (nn.Module): module that takes the output of box_head and returns the\n",
    "            classification logits and box regression deltas.\n",
    "        box_score_thresh (float): during inference, only return proposals with a classification score\n",
    "            greater than box_score_thresh\n",
    "        box_nms_thresh (float): NMS threshold for the prediction head. Used during inference\n",
    "        box_detections_per_img (int): maximum number of detections per image, for all classes.\n",
    "        box_fg_iou_thresh (float): minimum IoU between the proposals and the GT box so that they can be\n",
    "            considered as positive during training of the classification head\n",
    "        box_bg_iou_thresh (float): maximum IoU between the proposals and the GT box so that they can be\n",
    "            considered as negative during training of the classification head\n",
    "        box_batch_size_per_image (int): number of proposals that are sampled during training of the\n",
    "            classification head\n",
    "        box_positive_fraction (float): proportion of positive proposals in a mini-batch during training\n",
    "            of the classification head\n",
    "        bbox_reg_weights (Tuple[float, float, float, float]): weights for the encoding/decoding of the\n",
    "            bounding boxes\n",
    "        mask_roi_pool (MultiScaleRoIAlign): the module which crops and resizes the feature maps in\n",
    "             the locations indicated by the bounding boxes, which will be used for the mask head.\n",
    "        mask_head (nn.Module): module that takes the cropped feature maps as input\n",
    "        mask_predictor (nn.Module): module that takes the output of the mask_head and returns the\n",
    "            segmentation mask logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf025fc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T18:08:25.927967Z",
     "start_time": "2025-12-01T18:08:25.913503Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "\n",
    "### with help of https://docs.pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n",
    "\n",
    "#TODO : DANS LE modèle rajouter comme variable box_score_thresh\n",
    "def get_model_instance_segmentation(num_classes, box_score_thresh=0.05, min_size=800, max_size=1333):\n",
    "    # load pretrained maskrcnn\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\", \n",
    "                                                               box_score_thresh=box_score_thresh, \n",
    "                                                               min_size=min_size,\n",
    "                                                               max_size= max_size)\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features  # type: ignore\n",
    "    # replace the pre-trained head\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # get number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels  # type: ignore\n",
    "    hidden_layer = 256\n",
    "    # replace the mask predictor\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "#TODO : enlever ça je pense c'est impossible à utilise at au vu de nos résultats pas besoin\n",
    "class MaskRCNNWrapper(torch.nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.model = get_model_instance_segmentation(num_classes)\n",
    "\n",
    "    def forward(self, images, targets=None):\n",
    "        return self.model(images, targets)\n",
    "\n",
    "    def freeze(self):\n",
    "        \"\"\"\n",
    "        Freezes backbone layer learning.\n",
    "        To prevent unlearning learned features.\n",
    "        \"\"\"\n",
    "        for param in self.model.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze(self):\n",
    "        \"\"\"\n",
    "        Unfreezes backbone layer learning.\n",
    "        \"\"\"\n",
    "        for param in self.model.backbone.parameters():\n",
    "            param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b7c2f6",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c301cbb",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430a98f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T18:08:26.011817Z",
     "start_time": "2025-12-01T18:08:25.928598Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2 as T\n",
    "\n",
    "\n",
    "def get_transform(training=True):\n",
    "    \"\"\"\n",
    "    Returns a composition of all desired torchvision.transforms to apply during data preparation.\n",
    "    training (optional, True as default) : differentiates the transforms to apply to the data\n",
    "    meant for training and those meant for validation phase.\n",
    "    \"\"\"\n",
    "    transforms = []\n",
    "    if training:\n",
    "        transforms.append(T.RandomHorizontalFlip())\n",
    "    transforms.append(T.ToDtype(torch.float32, scale=True))\n",
    "    transforms.append(T.ToPureTensor())\n",
    "    return T.Compose(transforms)\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to prevent stacking of images with different shapes\n",
    "    \"\"\"\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "def get_filtered_cat_ids(cats, names):\n",
    "    \"\"\"\n",
    "    Get a list of ids of given list of categories\n",
    "    \"\"\"\n",
    "    cat_ids = []\n",
    "    cat_names = []\n",
    "    for id, cat in cats.items():\n",
    "        if cat['name'] in names:\n",
    "            cat_ids.append(id)\n",
    "            cat_names.append(cat['name'])\n",
    "    print(f'category found for {[name for name in names if name in cat_names]}')\n",
    "    print(f'category NOT found for {[name for name in names if name not in cat_names]}\\n')\n",
    "    return cat_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880a898d",
   "metadata": {},
   "source": [
    "#### IoU and Dice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93d3909",
   "metadata": {},
   "source": [
    "Inspired by https://docs.pytorch.org/vision/main/_modules/torchvision/ops/boxes.html#box_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e440b51b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T19:31:18.624648Z",
     "start_time": "2025-12-01T19:31:18.606163Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_iou_matrix(p, t):\n",
    "    \"\"\"\n",
    "    Returns a Tensor representing the correlation between all predictions and all targets.\n",
    "    \"\"\"\n",
    "    # pred masks : torch.Size([79, 1, 453, 640])\n",
    "    pred_masks = p['masks'].squeeze(1)\n",
    "    target_masks = t['masks']\n",
    "\n",
    "    if pred_masks.size(0) == 0 or target_masks.size(0) == 0:\n",
    "        print(f\"compute_iou_matrix : pas de predictions ou pas de targets\")\n",
    "        return None\n",
    "\n",
    "    # We need to binarize to perform scalar product\n",
    "    pred_masks_bin = (pred_masks > 0.5).float()\n",
    "    target_masks = target_masks.float()\n",
    "\n",
    "    # We reshape preds and targets into matrices to perform scalar product\n",
    "    N = pred_masks_bin.size(0)\n",
    "    M = target_masks.size(0)\n",
    "\n",
    "    # (N,H,W) -> (N,H*W)\n",
    "    pred_matrix = pred_masks_bin.reshape(N, -1)\n",
    "    # (M,H,W) -> (M, H*W)\n",
    "    target_matrix = target_masks.reshape(M, -1)\n",
    "\n",
    "    # Since binary, the intersection is the scalar product between the two matrices \n",
    "    # We take the transpose of target_matrix. (N,H*W)*(H*W,M) = (N,M)\n",
    "    intersection = torch.mm(pred_matrix, target_matrix.t())\n",
    "\n",
    "    # we count the number of pixels (area) for each predicted instances and targets.\n",
    "    pred_area = pred_matrix.sum(axis=1, keepdims=True)  # (N,1)\n",
    "    target_area = target_matrix.sum(axis=1, keepdims=True)  # (M,1)\n",
    "    # \n",
    "    broadcast_matrix = pred_area + target_area.t()\n",
    "\n",
    "    # Union =  A + B - intersection\n",
    "    union = broadcast_matrix - intersection\n",
    "\n",
    "    iou_matrix = intersection / (union + 1e-6)\n",
    "\n",
    "    # We sort the matrix by lines (predictions) based on 'score' \n",
    "    # so the calculations on tp,fp,fn are more pertinent\n",
    "    sorted_indices = torch.argsort(p['scores'], descending=True)\n",
    "    iou_matrix_sorted = iou_matrix[sorted_indices]\n",
    "\n",
    "    return iou_matrix_sorted\n",
    "\n",
    "\n",
    "def count_tp_fp_fn(p,t, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calculate from the number of true positives, false positives and false negatives.\n",
    "    Based on the calculation of a iou matrix see compute_iou_matrix(p,t) function\n",
    "    \"\"\"\n",
    "    iou_matrix = compute_iou_matrix(p,t)\n",
    "    N, M = iou_matrix.shape\n",
    "    if iou_matrix is None:\n",
    "        return 0, N, M  # if None no tp\n",
    "\n",
    "    # We keep the already detected matches, for us if a target is detected\n",
    "    # twice (two separate prediction masks) then the second detection is a false positive.\n",
    "    detected_matches = [] \n",
    "    # True positives and false positves count\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "\n",
    "    # We search for each predicted mask if it corresponds to a target\n",
    "    for id_predMask in range(N):\n",
    "        # if no targets then only false positives\n",
    "        if M == 0:\n",
    "            fp += 1\n",
    "            continue\n",
    "\n",
    "        # Using the iou_matrix we find the best possible candidate to be the target for our predicted mask\n",
    "        best_target_candidate = torch.argmax(iou_matrix[id_predMask]).item()\n",
    "        best_iou = iou_matrix[id_predMask][best_target_candidate]\n",
    "        # If the iou between our predicted mask and its best corresponding target is higher\n",
    "        # than a threshold we consider it as a true positive.\n",
    "        if best_iou > iou_threshold:\n",
    "            if best_target_candidate not in detected_matches:\n",
    "                tp += 1\n",
    "                detected_matches.append(best_target_candidate)\n",
    "            else:\n",
    "                # already detected once\n",
    "                fp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    # False negatives count\n",
    "    fn = M - tp\n",
    "    return tp, fp, fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b45f62d",
   "metadata": {},
   "source": [
    "#### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698a8e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_img_and_mask(ax, mask, name, color):\n",
    "    ys, xs = np.nonzero(mask)\n",
    "    y, x = ys.mean(), xs.mean()\n",
    "    overlay = np.zeros((*mask.shape, 4))\n",
    "    overlay[..., :3] = color[:3]  # apply colors except alphas\n",
    "    overlay[..., 3] = mask * 0.5  # alpha is =0.5 where mask exists\n",
    "    ax.imshow(overlay)\n",
    "    ax.text(x, y, name, color=\"white\", ha=\"center\", va=\"center\")\n",
    "\n",
    "\n",
    "def plot_images_with_anns(img, target, pred, label_to_name=None, score_thresh=0.3):\n",
    "    img_np = img.permute(1, 2, 0).cpu().numpy()\n",
    "    pmasks = pred['masks'].cpu().numpy()[:, 0, :, :]\n",
    "    pscores = pred[\"scores\"].cpu().numpy()\n",
    "    pmasks = (pmasks > 0.5).astype(np.uint8)  # to binary mask\n",
    "    plabels = pred['labels'].cpu().numpy()\n",
    "    tmasks = target['masks'].cpu().numpy()\n",
    "    tlabels = target['labels'].cpu().numpy()\n",
    "\n",
    "    _, axes = plt.subplots(1, 2, figsize=(8, 8))\n",
    "    ax_l, ax_r = axes\n",
    "    ax_l.imshow(img_np.astype(np.float32))\n",
    "    ax_r.imshow(img_np.astype(np.float32))\n",
    "    for tlabel, tmask in zip(tlabels, tmasks):\n",
    "        if not np.any(tmask):\n",
    "            continue  # skip empty mask\n",
    "        label_name = label_to_name[tlabel] if label_to_name is not None and tlabel in label_to_name else str(tlabel)\n",
    "        color = plt .get_cmap(\"tab20\")(tlabel % 20)\n",
    "        add_img_and_mask(ax_l, tmask, label_name, color)\n",
    "    for plabel, pmask, pscore in zip(plabels, pmasks, pscores):\n",
    "        if not np.any(pmask) or pscore < score_thresh:\n",
    "            continue  # skip empty mask or low score\n",
    "        label_name = label_to_name[plabel] if label_to_name is not None and plabel in label_to_name else str(plabel)\n",
    "        color = plt.get_cmap(\"tab20\")(plabel % 20)\n",
    "        add_img_and_mask(ax_r, pmask, label_name, color)\n",
    "    ax_l.set_axis_off()\n",
    "    ax_r.set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_losses(train_losses, val_losses,best_model_epoch=None,early_stop=None, filepath=None):\n",
    "    _, ax = plt.subplots(figsize=((12, 6)))\n",
    "    epochs = np.arange(1, len(train_losses) + 1, 1)\n",
    "    ax.plot(epochs, train_losses, 'r', label='Training Loss')\n",
    "    ax.plot(epochs, val_losses, 'g', label='Validation Loss')\n",
    "    if early_stop is not None:\n",
    "        plt.scatter(epochs[early_stop], val_losses[early_stop], marker='x', c='g', label='start of overfitting')\n",
    "    if best_model_epoch is not None:\n",
    "        plt.scatter(epochs[best_model_epoch], val_losses[best_model_epoch], marker='x', c='b', label='Saved Model Epoch')\n",
    "    ax.set_title('Loss Plots')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.legend(loc=\"upper right\")\n",
    "    if filepath is not None:\n",
    "        plt.savefig(filepath)\n",
    "    plt.show()\n",
    "        \n",
    "def plot_iou(val_iou, best_model_epoch=None,early_stop=None, filepath=None):\n",
    "    _, ax = plt.subplots(figsize=((12, 6)))\n",
    "    epochs = np.arange(1, len(val_iou) + 1, 1)\n",
    "    ax.plot(epochs, val_iou, 'r', label='val iou')\n",
    "\n",
    "    if early_stop is not None:\n",
    "        plt.scatter(epochs[early_stop], val_iou[early_stop], marker='x', c='g', label='Saved Model Epoch')\n",
    "    if best_model_epoch is not None:\n",
    "        plt.scatter(epochs[best_model_epoch], val_iou[best_model_epoch], marker='x', c='b', label='Saved Model Epoch')\n",
    "    ax.set_title('iou Plot')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('iou')\n",
    "    ax.legend(loc=\"upper right\")\n",
    "    if filepath is not None:\n",
    "        plt.savefig(filepath)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db8b1a4",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbef985d",
   "metadata": {},
   "source": [
    "pred = dict_keys(['boxes', 'labels', 'scores', 'masks'])\n",
    "pred masks torch.Size([79, 1, 453, 640])\n",
    "\n",
    "target : {'image_id': 38438, 'labels': tensor([1]), 'area': tensor([51464.3008]), 'boxes': tensor([[ 16.2200,  16.1600, 316.9400, 345.2500]]), 'masks': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
    "         [0, 0, 0,  ..., 0, 0, 0],\n",
    "...\n",
    "         [0, 0, 0,  ..., 0, 0, 0],\n",
    "         [0, 0, 0,  ..., 0, 0, 0],\n",
    "         [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178314fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T19:31:13.996439Z",
     "start_time": "2025-12-01T19:31:13.966900Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def train_one_epoch(model, optimizer, data_loader, epoch, device, scaler=None,\n",
    "                    warmup=True,\n",
    "                    print_freq: None | int = 10):\n",
    "    \"\"\"\n",
    "    Executes a training loop for a single epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    lr_warmup = None\n",
    "    if epoch==0 and warmup:\n",
    "        warmup_iters = min(1000, len(data_loader) - 1)\n",
    "        warmup_factor = 1.0 / warmup_iters\n",
    "        lr_warmup = torch.optim.lr_scheduler.LinearLR(\n",
    "            optimizer, start_factor=warmup_factor, total_iters=warmup_iters\n",
    "        )\n",
    "\n",
    "    total_loss = 0\n",
    "    for i, (images, targets) in enumerate(tqdm(data_loader, desc=\"TRAIN EPOCH (/batches)\")):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # We use automatic mixed precision for better performence.\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=scaler is not None):\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()  # type: ignore\n",
    "        total_loss += loss_value\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Since we use amp we need a scaler to work in float16 without risking losing information.\n",
    "        if scaler is not None:\n",
    "            scaler.scale(losses).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            losses.backward()  # type: ignore\n",
    "            optimizer.step()\n",
    "\n",
    "        if print_freq is not None and i % print_freq == 0:\n",
    "            tqdm.write(f\"[batch {i + 1}/{len(data_loader)}] loss: {loss_value}\")\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(f\"Loss is {loss_value}, stopping training \\nLoss dict:\\n{loss_dict}\")\n",
    "            sys.exit(1)\n",
    "        if lr_warmup is not None:\n",
    "            lr_warmup.step()\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader, device, iou_thresh=0.5):\n",
    "    \"\"\"\n",
    "    Evaluates the model.\n",
    "    Contains two loops over the data. First to calculate the loss and second to calculate the other metrics such as IoU.\n",
    "    \"\"\"\n",
    "    model.train()   # in train to output validation loss\n",
    "    total_loss = 0\n",
    "    for images, targets in tqdm(data_loader, desc=\"VALIDATION loss (/batches)\"):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n",
    "        # get loss\n",
    "        loss_dict = model(images, targets)\n",
    "        total_loss += sum(loss.item() for loss in loss_dict.values())\n",
    "\n",
    "    model.eval()    # in eval to output predictions\n",
    "    # true positives, false positives, false negatives\n",
    "    total_tp, total_fp, total_fn = 0, 0, 0\n",
    "    for images, targets in tqdm(data_loader, desc=\"VALIDATION preds (/batches)\"):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n",
    "        preds = model(images)\n",
    "        # Count the tp, fp, fn for each image masks predicted based on associated target\n",
    "        for p, t in zip(preds, targets):\n",
    "            tp, fp, fn = count_tp_fp_fn(p, t, iou_thresh)\n",
    "            total_tp += tp\n",
    "            total_fp += fp\n",
    "            total_fn += fn\n",
    "\n",
    "    eps = 1e-6  # epsilon to avoid division with zero\n",
    "    iou = total_tp / (total_tp + total_fp + total_fn + eps)\n",
    "    precision = total_tp / (total_tp + total_fp + eps)\n",
    "    recall = total_tp / (total_tp + total_fn + eps)\n",
    "    f1 = 2 * precision * recall / (precision + recall + eps)\n",
    "    val_loss = total_loss / len(data_loader)\n",
    "    return val_loss, iou, precision, recall, f1\n",
    "\n",
    "\n",
    "def train(\n",
    "        model, optimizer, lr_scheduler,\n",
    "        train_loader, val_loader, lvis_gt_val,\n",
    "        epochs, patience, warmup,\n",
    "        save_model_path, device, scaler=None,\n",
    "        print_freq: None | int = 10\n",
    "):\n",
    "    \"\"\"\n",
    "    Main training loop.\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_iou = []\n",
    "\n",
    "    # Tracking\n",
    "    early_stoping_epoch = 0\n",
    "    best_model_epoch = 0\n",
    "    epochs_no_improvement = 0\n",
    "    best_iou = 0\n",
    "\n",
    "    # Example images to display during training\n",
    "    images_vis, targets_vis = next(iter(val_loader))\n",
    "    images_vis = list(image.to(device) for image in images_vis)\n",
    "    targets_vis = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets_vis]\n",
    "    label_to_name = {k: lvis_gt_val.load_cats([v])[0]['name'] for k, v in\n",
    "                                 val_loader.dataset.dataset.label_to_cat_id.items()}\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc='TRAINING LOOP (/epochs)'):\n",
    "\n",
    "        loss_train = train_one_epoch(model, optimizer, train_loader, epoch, device, scaler, warmup, print_freq)\n",
    "        train_losses.append(loss_train)\n",
    "\n",
    "        val_loss, iou, precision, recall, f1 = evaluate(model, val_loader, device)\n",
    "        val_losses.append(val_loss)\n",
    "        val_iou.append(iou)\n",
    "        tqdm.write(f\"[epoch {str(epoch + 1).zfill(2)}/{str(epochs).zfill(2)}]: train loss = {loss_train:.4f} | val loss = {val_loss:.4f}\")\n",
    "        tqdm.write(f\"               iou = {iou:.4f} | precision = {precision:.4f} | recall = {recall:.4f} | f1 = {f1:.4f}\")\n",
    "\n",
    "        #To display an example\n",
    "        #TODO : Change %1 (test)\n",
    "        if (epoch + 1) % 1 == 0:\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                preds_vis = model(images_vis)  \n",
    "            plot_images_with_anns(images_vis[0], targets_vis[0], preds_vis[0], label_to_name, 0.5)\n",
    "        \n",
    "        if iou >= best_iou: \n",
    "            best_iou = iou\n",
    "            best_model_epoch = epoch\n",
    "            epochs_no_improvement = 0\n",
    "            torch.save(model.state_dict(), save_model_path)\n",
    "        else:\n",
    "            epochs_no_improvement += 1\n",
    "            tqdm.write(f\"NO improvement [{epochs_no_improvement}/{patience}]\")\n",
    "            if epochs_no_improvement >= patience:\n",
    "                print(\"Patience reached, stopping training\")\n",
    "                break\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "    return train_losses, val_losses,val_iou, best_model_epoch, early_stoping_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6806c8d0",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df378ddf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T19:54:01.819190Z",
     "start_time": "2025-12-01T19:54:01.801724Z"
    }
   },
   "outputs": [],
   "source": [
    "from lvis import LVISResults, LVISEval\n",
    "import pycocotools.mask as maskUtils\n",
    "\n",
    "\n",
    "# TODO masks do not seem to be in the right format\n",
    "@torch.inference_mode()\n",
    "def get_predictions(model, data_loader, device, score_thresh=0.05):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    for images, targets in tqdm(data_loader, desc=\"BATCHES\"):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        outputs = model(images)\n",
    "        for o, t in zip(outputs, targets):\n",
    "            image_id = t[\"image_id\"]\n",
    "            boxes = o[\"boxes\"].cpu().numpy()\n",
    "            scores = o[\"scores\"].cpu().numpy()\n",
    "            labels = o[\"labels\"].cpu().numpy()\n",
    "            masks = o[\"masks\"].cpu().numpy()[:, 0, :, :]\n",
    "            for box, score, label, mask in zip(boxes, scores, labels, masks):\n",
    "                if score < score_thresh:\n",
    "                    continue\n",
    "                # Convert mask to binary on the 0.5 threshold\n",
    "                mask_bin = (mask > 0.5).astype(np.uint8)\n",
    "                # LVIS requires Run Lenght Encoding for masks\n",
    "                rle = maskUtils.encode(np.asfortranarray(mask_bin))\n",
    "                rle['counts'] = rle['counts'].decode('utf-8')\n",
    "                # LVIS needs [x,y,width,height] format for boxes\n",
    "                x1, y1, x2, y2 = box\n",
    "                predictions.append({\n",
    "                    \"image_id\": image_id,\n",
    "                    \"category_id\": data_loader.dataset.dataset.label_to_cat_id[int(label)],\n",
    "                    # dict inside dataset in subset in loader\n",
    "                    \"bbox\": [float(x1), float(y1), float(x2 - x1), float(y2 - y1)],\n",
    "                    \"segmentation\": rle,\n",
    "                    \"score\": float(score)\n",
    "                })\n",
    "    return predictions\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def run_lvis_eval(model, data_loader, lvis_gt, cat_ids, device, iou_type: Literal[\"bbox\", \"segm\"] = \"segm\"):\n",
    "    score_thresh = 0.0001   # very low threshold for lviseval\n",
    "    predictions = get_predictions(model, data_loader, device, score_thresh)\n",
    "    if len(predictions) == 0:\n",
    "        print(\"No detections — skipping LVIS evaluation.\")\n",
    "    else:\n",
    "        lvis_dt = LVISResults(lvis_gt, predictions)\n",
    "        lvis_eval = LVISEval(lvis_gt, lvis_dt, iou_type)\n",
    "        lvis_eval.params.cat_ids = cat_ids\n",
    "        lvis_eval.run()\n",
    "        lvis_eval.print_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4c5a34",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa22da0f",
   "metadata": {},
   "source": [
    "#### Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf10d76a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T18:08:26.123159Z",
     "start_time": "2025-12-01T18:08:26.107938Z"
    }
   },
   "outputs": [],
   "source": [
    "### Device\n",
    "NO_AMP: bool = False\n",
    "PIN_MEMORY: bool = True\n",
    "NUM_WORKERS: int = 4  # int (0: main process)\n",
    "\n",
    "### Data\n",
    "MAX_IMAGES: int = 10000  # 80/20 split (train: 0.8*max_img | val: 0.2*max_img)\n",
    "CATEGORIES: List[str] = ['cat', 'dog', 'cow', 'bull', 'pigeon', 'giraffe', 'bear', 'elephant', 'rabbit', 'horse']\n",
    "\n",
    "### Model\n",
    "MIN_IMG_SIZE: int = 400  # 800 default\n",
    "MAX_IMG_SIZE: int = 800 # 1333 default\n",
    "BOX_SCORE_THRESH: float = 0.1   # 0.05 default\n",
    "\n",
    "### Learning\n",
    "LR: float = 1e-3\n",
    "MOMENTUM: float = 0.9\n",
    "EPOCHS: int = 25\n",
    "BATCH_SIZE: int = 10\n",
    "PATIENCE: int = 5\n",
    "WARMUP: bool = True      # whether the learning rate should start small during the first epoch\n",
    "\n",
    "### Others\n",
    "BATCH_PRINT_FREQ: int|None = None  # None: no print inside epoch\n",
    "OUTPUT_DIR: str = 'output' # '/kaggle/working/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4105a186",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d3fc3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T18:09:52.120222Z",
     "start_time": "2025-12-01T18:08:26.127405Z"
    }
   },
   "outputs": [],
   "source": [
    "### load annotations\n",
    "from lvis import LVIS\n",
    "\n",
    "lvis_gt_train = LVIS(TRAIN_ANNOT_PATH)\n",
    "lvis_gt_val = LVIS(VAL_ANNOT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3224df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T18:09:52.252407Z",
     "start_time": "2025-12-01T18:09:52.127792Z"
    }
   },
   "outputs": [],
   "source": [
    "### create datasets/dataloaders\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "cat_ids = get_filtered_cat_ids(lvis_gt_train.cats, CATEGORIES)\n",
    "num_classes = len(cat_ids) + 1  # +1 for background\n",
    "\n",
    "# train\n",
    "dataset_train = LVISDataset([COCO2017_TRAIN_PATH], lvis_gt_train, get_transform(training=True), cat_ids)\n",
    "subset_train = Subset(dataset_train, (torch.randperm(len(dataset_train))[:int(MAX_IMAGES * 0.8)]).tolist())\n",
    "train_loader = DataLoader(subset_train,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          collate_fn=custom_collate_fn,\n",
    "                          shuffle=True,\n",
    "                          num_workers=NUM_WORKERS,\n",
    "                          pin_memory=PIN_MEMORY)\n",
    "print(f\"Size of train dataset: {len(dataset_train)}\")\n",
    "print(f\"Size of train subset: {len(subset_train)}\\n\")\n",
    "\n",
    "# val\n",
    "dataset_val = LVISDataset([COCO2017_VAL_PATH, COCO2017_TRAIN_PATH], lvis_gt_val,\n",
    "                          get_transform(training=False), cat_ids)\n",
    "subset_val = Subset(dataset_val, (torch.randperm(len(dataset_val))[:int(MAX_IMAGES * 0.2)]).tolist())\n",
    "val_loader = DataLoader(subset_val,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        collate_fn=custom_collate_fn,\n",
    "                        num_workers=NUM_WORKERS,\n",
    "                        pin_memory=PIN_MEMORY)\n",
    "print(f\"Size of validation dataset: {len(dataset_val)}\")\n",
    "print(f\"Size of validation subset: {len(subset_val)}\")\n",
    "\n",
    "# for faster evaluation (when instancing LVISResults):\n",
    "lvis_gt_val.cats = {k: v for k, v in lvis_gt_val.cats.items() if k in cat_ids}\n",
    "lvis_gt_train.cats = {k: v for k, v in lvis_gt_train.cats.items() if k in cat_ids}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca722e51",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a2c17c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T18:09:53.782599Z",
     "start_time": "2025-12-01T18:09:52.257542Z"
    }
   },
   "outputs": [],
   "source": [
    "### Initialization\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "scaler = torch.amp.GradScaler('cuda') if device.type == 'cuda' and not NO_AMP else None\n",
    "\n",
    "model = get_model_instance_segmentation(num_classes, box_score_thresh=BOX_SCORE_THRESH, min_size=MIN_IMG_SIZE, max_size=MAX_IMG_SIZE)\n",
    "model.to(device)\n",
    "print(f'Device used: {device.type}')\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(\n",
    "    params,\n",
    "    lr=LR,\n",
    "    momentum=MOMENTUM,\n",
    "    weight_decay=0.0005\n",
    ")\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=1,\n",
    "    gamma=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928419a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T19:55:35.040995Z",
     "start_time": "2025-12-01T19:54:11.370651Z"
    }
   },
   "outputs": [],
   "source": [
    "### Training Loop\n",
    "train_losses, val_losses,val_iou, best_epoch, early_stop_epoch = train(model,\n",
    "                                             optimizer, lr_scheduler,\n",
    "                                             train_loader, val_loader, lvis_gt_val,\n",
    "                                             EPOCHS, PATIENCE, WARMUP, \n",
    "                                             os.path.join(OUTPUT_DIR, \"best_model.pt\"),\n",
    "                                             device, scaler, BATCH_PRINT_FREQ)\n",
    "plot_losses(train_losses, val_losses, best_model_epoch=best_epoch,early_stop=early_stop_epoch, filepath=os.path.join(OUTPUT_DIR,\"losses.jpg\"))\n",
    "plot_iou(val_iou, early_stop=best_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9af7424",
   "metadata": {},
   "source": [
    "#### Evaluation & Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1ebdc3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T18:12:54.247469Z",
     "start_time": "2025-12-01T18:12:53.176591Z"
    }
   },
   "outputs": [],
   "source": [
    "### load best model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "best_model = get_model_instance_segmentation(num_classes)\n",
    "best_model.load_state_dict(torch.load(os.path.join(OUTPUT_DIR, \"best_model.pt\"), map_location=device))\n",
    "best_model.to(device)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0945571",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T18:12:57.996494Z",
     "start_time": "2025-12-01T18:12:54.247469Z"
    }
   },
   "outputs": [],
   "source": [
    "### run full LVIS evaluation\n",
    "run_lvis_eval(best_model, val_loader, lvis_gt_val, cat_ids, device, iou_type=\"segm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dee2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### evaluation metrics\n",
    "iou_threshs = [0.5, 0.75]\n",
    "for iou_thresh in iou_threshs:\n",
    "    val_loss, iou, precision, recall, f1 = evaluate(model, val_loader, device, iou_thresh)\n",
    "    print(f'For IoU = {iou_thresh}')\n",
    "    print(f'    Val loss  = {val_loss:.4f}')\n",
    "    print(f'    IoU       = {iou:.4f}')\n",
    "    print(f'    precision = {precision:.4f}')\n",
    "    print(f'    recall    = {recall:.4f}')\n",
    "    print(f'    f1        = {f1:.4f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a45331",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T18:13:01.933520Z",
     "start_time": "2025-12-01T18:12:58.006598Z"
    }
   },
   "outputs": [],
   "source": [
    "### visualization\n",
    "IMAGES_TO_SHOW = 5\n",
    "SCORE_THRESH = 0.3  # higher score thresh for visualisation\n",
    "\n",
    "label_to_name = {k: lvis_gt_val.load_cats([v])[0]['name'] for k, v in\n",
    "                 val_loader.dataset.dataset.label_to_cat_id.items()}\n",
    "\n",
    "images, targets = next(iter(val_loader))\n",
    "images = [img.to(device) for img in images]\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = best_model(images)\n",
    "for i in range(min(IMAGES_TO_SHOW, len(images))):\n",
    "    plot_images_with_anns(images[i], targets[i], preds[i], label_to_name, SCORE_THRESH)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
